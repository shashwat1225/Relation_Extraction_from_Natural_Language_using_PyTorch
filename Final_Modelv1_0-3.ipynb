{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook Link: https://colab.research.google.com/drive/1oBxzMsdIsYLikfExtrFd7T9vOvaCLaa_?usp=sharing\n",
        "\n",
        "codalab username: shashwat.pandey\n",
        "\n",
        "Student ID: spandey7"
      ],
      "metadata": {
        "id": "mKeaiw1TyCVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "ENBso28lF7QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "gYgJXGOHOSpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "msMXKrPjgfrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Dataset**"
      ],
      "metadata": {
        "id": "C3XagHONGB7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/hw1_train-1.csv', index_col = 0)\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/hw1_test-2.csv', index_col = 0)\n",
        "\n",
        "data.columns = [\"text\", \"labels\"]\n",
        "test_data.columns = [\"text\"]\n",
        "data[\"labels\"] = data[\"labels\"].str.replace(\"none\", \"\")\n",
        "data['labels'] = data['labels'].replace(np.nan,\"\")\n",
        "\n",
        "labels = [str(i).split() for i in data['labels']]\n",
        "text= data['text']"
      ],
      "metadata": {
        "id": "gLSy1MsXY-Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Preparation**"
      ],
      "metadata": {
        "id": "ph254jbEFOa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfvectorizer = TfidfVectorizer(max_features=3000)\n",
        "x_tfidf = tfidfvectorizer.fit_transform(text).toarray()\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(labels)\n",
        "Y = mlb.transform(labels)\n",
        "n_op_features = len(Y[0])\n",
        "train_x,test_x,train_y,test_y = train_test_split(x_tfidf,Y,test_size=0.01)\n",
        "n_ip_features = len(train_x[0])"
      ],
      "metadata": {
        "id": "NToUYSj4ZD54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    #Converting arrays to tensors of torch\n",
        "    self.X = torch.tensor(X)\n",
        "    self.y = torch.tensor(y)\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index], self.y[index]"
      ],
      "metadata": {
        "id": "BKAVAGMTZFV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "train_ds = MovieDataset(X=train_x, y=train_y)\n",
        "test_ds = MovieDataset(X=test_x, y=test_y)\n",
        "#DataLoader Definition\n",
        "dataloader_train = DataLoader(dataset=train_ds,batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "3tyLT_A3ZIc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model and Training**"
      ],
      "metadata": {
        "id": "PHkv59rgFVoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  #a multi-layered perceptron based classifier\n",
        "    def __init__(self, num_features,out_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features (int): the size of the input feature vector\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=num_features, out_features=64)\n",
        "        print(\"num f:\", num_features)\n",
        "        self.fc2 = nn.Linear(in_features=64,out_features=out_features)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, num_features)\n",
        "            apply_softmax (bool): a flag for the sigmoid activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,)\n",
        "        \"\"\"\n",
        "        y_out = torch.relu(self.fc1(x_in))\n",
        "        y_out = self.fc2(y_out)\n",
        "        return y_out"
      ],
      "metadata": {
        "id": "HaSM4_yqZOW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01\n",
        "num_epochs=100\n",
        "\n",
        "epoch_loss_list=[]\n",
        "epoch_acc_list=[]\n",
        "val_epoch_acc_list=[]\n",
        "val_epoch_loss_list=[]\n",
        "\n",
        "model = MLP(n_ip_features,n_op_features)\n",
        "model.to(device)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_iter=math.ceil(len(train_ds)/batch_size)\n",
        "print(n_iter)\n",
        "\n",
        "losses = []\n",
        "from sklearn.metrics import accuracy_score\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc=0\n",
        "    val_epoch_loss=0\n",
        "    val_epoch_acc=0\n",
        "    for k,(X,y) in enumerate(dataloader_train):\n",
        "        # the training routine is these 5 steps:\n",
        "\n",
        "        # step 1. load the data\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # step 2. compute the output\n",
        "        y_pred = model(x_in=X.float())\n",
        "        y_1 = (y_pred).to('cpu').detach().numpy()\n",
        "        y_1=(np.array(y_1) >= 0)*1\n",
        "        y_0=y.to('cpu').detach().numpy()\n",
        "        acc = sum([(y_0[i]==y_1[i]).all()*1 for i in range(len(y_0))])\n",
        "        epoch_acc+= acc\n",
        "\n",
        "        # step 3. compute the loss\n",
        "        loss = loss_func(y_pred, y.squeeze(1).float())\n",
        "        epoch_loss+= loss.item()\n",
        "\n",
        "        # step 4. use loss to produce gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # step 5. use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "    epoch_loss = round(epoch_loss/(k+1),3)\n",
        "    epoch_loss_list.append(epoch_loss)\n",
        "    epoch_acc = round(epoch_acc/len(train_ds),3)\n",
        "    epoch_acc_list.append(epoch_acc)\n",
        "    \n",
        "    for k,(X,y) in enumerate(dataloader_test):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        y_pred = model(x_in=X.float())\n",
        "        y_1 = (y_pred).to('cpu').detach().numpy()\n",
        "        y_1=(np.array(y_1) >= 0)*1\n",
        "        y_0=y.to('cpu').detach().numpy()\n",
        "        val_acc = sum([(y_0[i]==y_1[i]).all()*1 for i in range(len(y_0))])\n",
        "        val_epoch_acc+=val_acc\n",
        "        loss = loss_func(y_pred, y.squeeze(1).float())\n",
        "        val_epoch_loss+= loss.item()\n",
        "    val_epoch_acc=round(val_epoch_acc/len(test_ds),3)\n",
        "    val_epoch_acc_list.append(val_epoch_acc)\n",
        "    val_epoch_loss = round(val_epoch_loss/(k+1),3)\n",
        "    val_epoch_loss_list.append(val_epoch_loss)\n",
        "    print('epoch : ' + str(epoch+1)+'/'+str(num_epochs))\n",
        "    print(\"-\"*40)\n",
        "    print('loss : ' + str(epoch_loss)+ ' \\t val loss : '+ str(val_epoch_loss)+ '\\nacc :' + str(epoch_acc)+ ' \\t val acc :' + str(val_epoch_acc))\n",
        "    print(\"+\"*40)  # -----------------------------------------\n",
        "    losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "wC699O9GZZAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'MLP-TFIDF')"
      ],
      "metadata": {
        "id": "uwtxZhIUgq1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_graph(plot_var,train_plot_list,val_plot_list):\n",
        "    epochs = len(train_plot_list)\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    if plot_var==\"accuracy\": plt.title(\"Train/Validation Accuracy\")\n",
        "    elif plot_var ==\"loss\" : plt.title(\"Train/Validation Loss\")\n",
        "    plt.plot(list(np.arange(epochs) + 1) , train_plot_list, label='train')\n",
        "    plt.plot(list(np.arange(epochs) + 1), val_plot_list, label='validation')\n",
        "    plt.xlabel('num_epochs', fontsize=12)\n",
        "    plt.ylabel('loss', fontsize=12)\n",
        "    plt.legend(loc='best')\n",
        "    if plot_var==\"accuracy\": plt.savefig('Train_Val_accuracy.png')\n",
        "    elif plot_var ==\"loss\" : plt.savefig(\"Train_Val_loss.png\")\n",
        "    return"
      ],
      "metadata": {
        "id": "stkkj-AiQhsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    plot_graph(\"accuracy\",epoch_acc_list, val_epoch_acc_list)\n",
        "    plot_graph(\"loss\",epoch_loss_list, val_epoch_loss_list)"
      ],
      "metadata": {
        "id": "xJDOPPb0Rj6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predictions**"
      ],
      "metadata": {
        "id": "npYR9-CiFagL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('MLP-TFIDF')\n",
        "def multilabel_predict(x):\n",
        "    x = tfidfvectorizer.transform([x]).toarray()\n",
        "    x = torch.tensor(x, dtype=torch.float64).cuda()\n",
        "    pred = model(x_in=x.float())\n",
        "    y_1 = (pred).to('cpu').detach().numpy()\n",
        "    y_1=(np.array(y_1) >= 0.9)*1\n",
        "    y_1 = mlb.inverse_transform(y_1)\n",
        "    return y_1[0]"
      ],
      "metadata": {
        "id": "qW9Yt0mgZaYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_utterences= test_data['text']\n",
        "predicted_relations=[]\n",
        "for utterence in test_utterences:\n",
        "    test_pred=multilabel_predict(utterence)\n",
        "    if len(test_pred)>0:\n",
        "        if len(test_pred)>1 and 'none' in test_pred:\n",
        "            test_pred=list(test_pred)\n",
        "            test_pred.remove('none')\n",
        "        predicted_relations.append((' ').join(sorted(test_pred)))\n"
      ],
      "metadata": {
        "id": "mYvmVrZvZjRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_list = list(range(len(predicted_relations)))\n",
        "final = pd.DataFrame(zip(id_list,predicted_relations), columns=['ID','CORE RELATIONS'])\n",
        "final[\"CORE RELATIONS\"] = final[\"CORE RELATIONS\"].str.replace(\"none\", \"\")\n",
        "final[\"CORE RELATIONS\"] = final[\"CORE RELATIONS\"].replace(np.nan, \"\")\n",
        "final.to_csv('submission.csv', index=None)"
      ],
      "metadata": {
        "id": "Fj8SnJVDZj7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}